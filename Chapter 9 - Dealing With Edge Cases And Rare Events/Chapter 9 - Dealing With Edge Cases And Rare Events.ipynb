{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030b2e16",
   "metadata": {},
   "source": [
    "## Technical Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71725d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2627c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyter\n",
    "# pip install snorkel\n",
    "# pip install scikit-learn\n",
    "# pip install Pillow\n",
    "# pip install tensorflow\n",
    "# pip install pandas\n",
    "# pip install numpy\n",
    "# pip install matplotlib\n",
    "# pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f4070",
   "metadata": {},
   "source": [
    "### Statistical Methods\n",
    "Statistical methods provide valuable tools for identifying outliers and anomalies in our data, aiding in data preprocessing and decision-making. In this section, we’ll talk about how to use methods like Z-Scores, IQR, box plots and scatter plots, to uncover anomalies in our data.\n",
    "\n",
    "#### Z-Score\n",
    "Z-scores, also known as standard scores, are a statistical measure that indicates how many standard deviations a data point is away from the mean of the data. Z-scores are used to standardize data and allow for comparisons between different datasets, even if they have different units or scales. They are particularly useful in detecting outliers and identifying extreme values in a dataset. The formula to calculate the Z-score for a data point x in a dataset with mean μ and standard deviation σ is given by:\n",
    "\n",
    "Z=(x-μ)/σ\n",
    "\n",
    "Where:\n",
    "\n",
    "Z is the Z-score of the data point x.\n",
    "x is the value of the data point.\n",
    "μ is the mean of the dataset.\n",
    "σ is the standard deviation of the dataset.\n",
    "\n",
    "Z-scores are widely used to detect outliers in a dataset. Data points with Z-scores that fall outside a certain threshold (e.g., Z > 3 or Z < -3) are considered outliers. These outliers can represent extreme values or measurement errors in the data. Z-scores are used to standardize data, making it comparable across different datasets. By transforming the data into Z-scores, the mean becomes 0, and the standard deviation becomes 1, resulting in a standardized distribution.\n",
    "\n",
    "Z-scores are employed in normality testing to assess whether a dataset follows a normal (Gaussian) distribution. If the dataset follows a normal distribution, approximately 68% of the data points should have Z-scores between -1 and 1, about 95% between -2 and 2, and nearly all between -3 and 3. In hypothesis testing, Z-scores are used to compute p-values and make inferences about population parameters. \n",
    "\n",
    "For example, Z-tests are commonly used for sample mean comparisons when the population standard deviation is known. Z-scores can be useful in anomaly detection where we want to identify data points that deviate significantly from the norm. High Z-scores may indicate anomalous behaviour or rare events in the data. \n",
    "\n",
    "Lets explore this concept in Python using the Loan Prediction dataset. Let's start by loading the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d358a2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loan_ID</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>LP001003</td>\n",
       "      <td>LP001005</td>\n",
       "      <td>LP001006</td>\n",
       "      <td>LP001008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <td>5849</td>\n",
       "      <td>4583</td>\n",
       "      <td>3000</td>\n",
       "      <td>2583</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>NaN</td>\n",
       "      <td>128.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>360.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Property_Area</th>\n",
       "      <td>Urban</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Status</th>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0         1         2             3         4\n",
       "Loan_ID            LP001002  LP001003  LP001005      LP001006  LP001008\n",
       "Gender                 Male      Male      Male          Male      Male\n",
       "Married                  No       Yes       Yes           Yes        No\n",
       "Dependents                0         1         0             0         0\n",
       "Education          Graduate  Graduate  Graduate  Not Graduate  Graduate\n",
       "Self_Employed            No        No       Yes            No        No\n",
       "ApplicantIncome        5849      4583      3000          2583      6000\n",
       "CoapplicantIncome       0.0    1508.0       0.0        2358.0       0.0\n",
       "LoanAmount              NaN     128.0      66.0         120.0     141.0\n",
       "Loan_Amount_Term      360.0     360.0     360.0         360.0     360.0\n",
       "Credit_History          1.0       1.0       1.0           1.0       1.0\n",
       "Property_Area         Urban     Rural     Urban         Urban     Urban\n",
       "Loan_Status               Y         N         Y             Y         Y"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('train_loan_prediction.csv')\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f6ca6",
   "metadata": {},
   "source": [
    "Now that we have loaded the dataset, we can calculate the Z-scores on some of the numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af69d35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LP001020</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>12841</td>\n",
       "      <td>10968.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>LP001448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>23803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>LP001469</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>20166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>LP001536</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>39999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>LP001585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>51763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>LP001610</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5516</td>\n",
       "      <td>11300.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>LP001637</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>33846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>LP001640</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>39147</td>\n",
       "      <td>4750.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>LP001907</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>14583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>436.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>LP001996</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>20233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>LP002101</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>LP002191</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>19730</td>\n",
       "      <td>5266.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>LP002297</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2500</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>LP002317</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>81000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>LP002342</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1600</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>LP002386</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>LP002422</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>37719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>LP002547</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>18333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>LP002624</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>20833</td>\n",
       "      <td>6667.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>LP002693</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7948</td>\n",
       "      <td>7166.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>LP002813</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>19484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>LP002893</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>1836</td>\n",
       "      <td>33837.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>LP002949</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416</td>\n",
       "      <td>41667.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Urban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>LP002959</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Loan_ID  Gender Married Dependents Education Self_Employed  \\\n",
       "9    LP001020    Male     Yes          1  Graduate            No   \n",
       "126  LP001448     NaN     Yes         3+  Graduate            No   \n",
       "130  LP001469    Male      No          0  Graduate           Yes   \n",
       "155  LP001536    Male     Yes         3+  Graduate            No   \n",
       "171  LP001585     NaN     Yes         3+  Graduate            No   \n",
       "177  LP001610    Male     Yes         3+  Graduate            No   \n",
       "183  LP001637    Male     Yes          1  Graduate            No   \n",
       "185  LP001640    Male     Yes          0  Graduate           Yes   \n",
       "278  LP001907    Male     Yes          0  Graduate            No   \n",
       "308  LP001996    Male      No          0  Graduate            No   \n",
       "333  LP002101    Male     Yes          0  Graduate           NaN   \n",
       "369  LP002191    Male     Yes          0  Graduate            No   \n",
       "402  LP002297    Male      No          0  Graduate            No   \n",
       "409  LP002317    Male     Yes         3+  Graduate            No   \n",
       "417  LP002342    Male     Yes          2  Graduate           Yes   \n",
       "432  LP002386    Male      No          0  Graduate           NaN   \n",
       "443  LP002422    Male      No          1  Graduate            No   \n",
       "487  LP002547    Male     Yes          1  Graduate            No   \n",
       "506  LP002624    Male     Yes          0  Graduate            No   \n",
       "523  LP002693    Male     Yes          2  Graduate           Yes   \n",
       "561  LP002813  Female     Yes          1  Graduate           Yes   \n",
       "581  LP002893    Male      No          0  Graduate            No   \n",
       "600  LP002949  Female      No         3+  Graduate           NaN   \n",
       "604  LP002959  Female     Yes          1  Graduate            No   \n",
       "\n",
       "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "9              12841            10968.0       349.0             360.0   \n",
       "126            23803                0.0       370.0             360.0   \n",
       "130            20166                0.0       650.0             480.0   \n",
       "155            39999                0.0       600.0             180.0   \n",
       "171            51763                0.0       700.0             300.0   \n",
       "177             5516            11300.0       495.0             360.0   \n",
       "183            33846                0.0       260.0             360.0   \n",
       "185            39147             4750.0       120.0             360.0   \n",
       "278            14583                0.0       436.0             360.0   \n",
       "308            20233                0.0       480.0             360.0   \n",
       "333            63337                0.0       490.0             180.0   \n",
       "369            19730             5266.0       570.0             360.0   \n",
       "402             2500            20000.0       103.0             360.0   \n",
       "409            81000                0.0       360.0             360.0   \n",
       "417             1600            20000.0       239.0             360.0   \n",
       "432            12876                0.0       405.0             360.0   \n",
       "443            37719                0.0       152.0             360.0   \n",
       "487            18333                0.0       500.0             360.0   \n",
       "506            20833             6667.0       480.0             360.0   \n",
       "523             7948             7166.0       480.0             360.0   \n",
       "561            19484                0.0       600.0             360.0   \n",
       "581             1836            33837.0        90.0             360.0   \n",
       "600              416            41667.0       350.0             180.0   \n",
       "604            12000                0.0       496.0             360.0   \n",
       "\n",
       "     Credit_History Property_Area Loan_Status  \n",
       "9               1.0     Semiurban           N  \n",
       "126             1.0         Rural           Y  \n",
       "130             NaN         Urban           Y  \n",
       "155             0.0     Semiurban           Y  \n",
       "171             1.0         Urban           Y  \n",
       "177             0.0     Semiurban           N  \n",
       "183             1.0     Semiurban           N  \n",
       "185             1.0     Semiurban           Y  \n",
       "278             1.0     Semiurban           Y  \n",
       "308             1.0         Rural           N  \n",
       "333             1.0         Urban           Y  \n",
       "369             1.0         Rural           N  \n",
       "402             1.0     Semiurban           Y  \n",
       "409             0.0         Rural           N  \n",
       "417             1.0         Urban           N  \n",
       "432             1.0     Semiurban           Y  \n",
       "443             1.0     Semiurban           Y  \n",
       "487             1.0         Urban           N  \n",
       "506             NaN         Urban           Y  \n",
       "523             1.0         Rural           Y  \n",
       "561             1.0     Semiurban           Y  \n",
       "581             1.0         Urban           N  \n",
       "600             NaN         Urban           N  \n",
       "604             1.0     Semiurban           Y  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
    "z_scores = df[numerical_features].apply(lambda x: (x - np.mean(x)) / np.std(x))\n",
    "#We will leverage Z-scores to detect outliers in our dataset effectively. \n",
    "threshold = 3\n",
    "outliers = (z_scores > threshold) | (z_scores < -threshold)\n",
    "outliers['is_outlier'] = outliers.any(axis=1)\n",
    "outlier_rows = df[outliers['is_outlier']]\n",
    "outlier_rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596f0ac",
   "metadata": {},
   "source": [
    "By setting a Z-score threshold of 3, we can identify outliers by examining if any rows have values that lie beyond the Z-score boundaries of greater than 3 or less than -3. This approach allows us to pinpoint data points that deviate significantly from the mean and enables us to take appropriate actions to handle these outliers, ensuring the integrity of our data and the accuracy of subsequent analyses and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957d865",
   "metadata": {},
   "source": [
    "##### IQR (Interquartile Range)\n",
    "Interquartile Range (IQR) is a statistical measure used to describe the spread or dispersion of a dataset. It is particularly useful in identifying and handling outliers and understanding the central tendency of the data. The IQR is defined as the range between the first quartile (Q1) and the third quartile (Q3) of a dataset. Quartiles are points that divide a dataset into four equal parts, each containing 25% of the data. \n",
    "\n",
    "The IQR can be calculated using the following formula:\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "Where:\n",
    "- Q1 is the first quartile (25th percentile), representing the value below which 25% of the data lies.\n",
    "- Q3 is the third quartile (75th percentile), representing the value below which 75% of the data lies.\n",
    "\n",
    "IQR is commonly used to identify outliers in a dataset. Data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers and may warrant further investigation. IQR provides valuable information about the distribution of the data. It helps to understand the spread of the middle 50% of the dataset and can be used to assess the symmetry of the distribution. When comparing datasets, IQR can be used to assess the differences in the spread of data between two or more datasets.\n",
    "\n",
    "Here's a simple example of how to calculate the IQR for a dataset using Python and NumPy on Loan Prediction dataset. To begin with outlier detection, we calculate the Interquartile Range (IQR) for numerical features. By defining a threshold, typically 1.5 times the Interquartile Range (IQR) based on Tukey's method, which involves calculating the IQR as the difference between the third quartile (Q3) and the first quartile (Q1), we can identify outliers. Tukey's method is a robust statistical technique that aids in detecting data points that deviate significantly from the overall distribution, providing a reliable measure for identifying potential anomalies in the dataset.. Once the outliers are flagged for all numerical features, we can display the rows with outlier values, which aids in further analysis and potential data treatment. Using the IQR and Tukey's method together facilitates effective outlier detection and helps ensure the integrity of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc50fd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LP001020</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>12841</td>\n",
       "      <td>10968.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LP001028</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3073</td>\n",
       "      <td>8106.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LP001046</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5955</td>\n",
       "      <td>5625.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LP001100</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>12500</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LP001114</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4166</td>\n",
       "      <td>7210.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>LP002893</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>1836</td>\n",
       "      <td>33837.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>LP002933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>LP002938</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>16120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>LP002949</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416</td>\n",
       "      <td>41667.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Urban</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>LP002959</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>12000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Loan_ID  Gender Married Dependents Education Self_Employed  \\\n",
       "9    LP001020    Male     Yes          1  Graduate            No   \n",
       "12   LP001028    Male     Yes          2  Graduate            No   \n",
       "21   LP001046    Male     Yes          1  Graduate            No   \n",
       "34   LP001100    Male      No         3+  Graduate            No   \n",
       "38   LP001114    Male      No          0  Graduate            No   \n",
       "..        ...     ...     ...        ...       ...           ...   \n",
       "581  LP002893    Male      No          0  Graduate            No   \n",
       "592  LP002933     NaN      No         3+  Graduate           Yes   \n",
       "594  LP002938    Male     Yes          0  Graduate           Yes   \n",
       "600  LP002949  Female      No         3+  Graduate           NaN   \n",
       "604  LP002959  Female     Yes          1  Graduate            No   \n",
       "\n",
       "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "9              12841            10968.0       349.0             360.0   \n",
       "12              3073             8106.0       200.0             360.0   \n",
       "21              5955             5625.0       315.0             360.0   \n",
       "34             12500             3000.0       320.0             360.0   \n",
       "38              4166             7210.0       184.0             360.0   \n",
       "..               ...                ...         ...               ...   \n",
       "581             1836            33837.0        90.0             360.0   \n",
       "592             9357                0.0       292.0             360.0   \n",
       "594            16120                0.0       260.0             360.0   \n",
       "600              416            41667.0       350.0             180.0   \n",
       "604            12000                0.0       496.0             360.0   \n",
       "\n",
       "     Credit_History Property_Area Loan_Status  \n",
       "9               1.0     Semiurban           N  \n",
       "12              1.0         Urban           Y  \n",
       "21              1.0         Urban           Y  \n",
       "34              1.0         Rural           N  \n",
       "38              1.0         Urban           Y  \n",
       "..              ...           ...         ...  \n",
       "581             1.0         Urban           N  \n",
       "592             1.0     Semiurban           Y  \n",
       "594             1.0         Urban           Y  \n",
       "600             NaN         Urban           N  \n",
       "604             1.0     Semiurban           Y  \n",
       "\n",
       "[77 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = df[numerical_features].quantile(0.25)\n",
    "Q3 = df[numerical_features].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "threshold = 1.5\n",
    "outliers = (df[numerical_features] < (Q1 - threshold * IQR)) | (df[numerical_features] > (Q3 + threshold * IQR))\n",
    "outliers['is_outlier'] = outliers.any(axis=1)\n",
    "outlier_rows = df[outliers['is_outlier']]\n",
    "outlier_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08d830",
   "metadata": {},
   "source": [
    "In conclusion, the exploration of Interquartile Range (IQR) provides a valuable technique for identifying outliers, particularly effective in capturing the central tendencies of a dataset. While IQR proves advantageous in scenarios where a focus on the middle range is paramount, it's essential to recognize its limitations in capturing the entire data distribution. In the upcoming section on Box-plot, we will delve into a graphical representation that complements IQR, offering a visual tool to better understand data dispersion and outliers in diverse contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785df944",
   "metadata": {},
   "source": [
    "#### Box-Plot\n",
    "Box plots, also known as box-and-whisker plots, are a graphical representation of the distribution of a dataset. They provide a quick and informative way to visualize the spread and skewness of the data, identify potential outliers, and compare multiple datasets. Box plots are particularly useful when dealing with continuous numerical data and can be used to gain insights into the central tendency and variability of the data.\n",
    "\n",
    "Here are the components of a box-plot: \n",
    "\n",
    "- Box (Interquartile Range, IQR): The box represents the Interquartile Range (IQR), which is the range between the first quartile (Q1) and the third quartile (Q3) of the data. It spans the middle 50% of the dataset and provides a visual representation of the data's spread.\n",
    "- Median (Q2): The median, represented by a horizontal line inside the box, indicates the central value of the dataset. It divides the data into two equal halves, with 50% of the data points below and 50% above the median.\n",
    "- Whiskers: The whiskers extend from the edges of the box to the furthest data points that lie within the \"whisker length.\" The length of the whiskers is typically determined by a factor (e.g., 1.5 times the IQR) and is used to identify potential outliers.\n",
    "- Outliers: Data points lying beyond the whiskers are considered outliers and are usually plotted individually as individual points or circles. They are data points that deviate significantly from the central distribution and may warrant further investigation.\n",
    "\n",
    "Following are the benefits of box-plots:\n",
    "\n",
    "- Visualizing Data Distribution: Box plots offer an intuitive way to see the spread and skewness of the data, as well as identify any potential data clusters or gaps.\n",
    "- Comparing Datasets: Box plots are useful for comparing multiple datasets side by side, allowing for easy comparisons of central tendencies and variabilities.\n",
    "- Outlier Detection: Box plots facilitate outlier detection by highlighting data points that lie beyond the whiskers, helping identify unusual or extreme values.\n",
    "- Handling Skewed Data: Box plots are robust to the influence of extreme values and can handle skewed data distributions more effectively than traditional mean and standard deviation.\n",
    "\n",
    "Let's implement this approach using the matplotlib library in Python. Here we will produce 2 box-plots for ‘ApplicantIncome’ and ‘LoanAmount’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cd7560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "df.boxplot(column='ApplicantIncome')\n",
    "plt.title('Box Plot - ApplicantIncome')\n",
    "plt.subplot(2, 1, 2)\n",
    "df.boxplot(column='LoanAmount')\n",
    "plt.title('Box Plot - LoanAmount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d591be",
   "metadata": {},
   "source": [
    "In summarizing the use of Box-plot, we find it to be a powerful visual aid, complementing IQR in portraying both central tendencies and data dispersion. While Box-plots excel in providing a holistic view, it's crucial to acknowledge that they may not capture all nuances of complex datasets. In the next section on Scatter plots, we will explore a versatile graphical tool that offers a broader perspective, facilitating the identification of relationships and patterns between variables in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4e215",
   "metadata": {},
   "source": [
    "#### Scatter Plot\n",
    "Scatter plots are a popular and versatile data visualization technique used to explore the relationship between two continuous numerical variables. They provide a clear visual representation of how one variable (the independent variable) affects or influences another (the dependent variable). Scatter plots are especially useful in identifying patterns, correlations, clusters, and outliers in the data, making them an essential tool in data analysis and exploratory data analysis (EDA).\n",
    "\n",
    "Here's how scatter plots are constructed and their key characteristics:\n",
    "\n",
    "Scatter Plot Construction\n",
    "\n",
    "To create a scatter plot, the values of the two numerical variables are plotted as points on a Cartesian coordinate system. Each point represents a data observation, where the x-coordinate corresponds to the value of the independent variable, and the y-coordinate corresponds to the value of the dependent variable. Multiple data points collectively form a scatter plot that provides insights into the relationship between the two variables.\n",
    "\n",
    "Key Characteristics of Scatter Plots\n",
    "\n",
    "Following are some of the key characteristics of scatter plots:\n",
    "\n",
    "- Correlation: Scatter plots help us assess the correlation or relationship between the two variables. If the points on the plot appear to form a clear trend or pattern (e.g., a linear or non-linear trend), it suggests a significant correlation between the variables. If the points are scattered randomly, there might be no or weak correlation.\n",
    "- Cluster Analysis: Scatter plots can reveal clusters of data points, indicating potential subgroups or patterns within the data.\n",
    "- Outlier Detection: Scatter plots facilitate outlier detection by identifying data points that lie far away from the main cluster of points.\n",
    "- Data Spread: The spread or distribution of the data points along the x and y axes provides insights into the variability of the variables.\n",
    "- Visualizing Regression Lines: In some cases, a regression line can be fitted to the scatter plot to model the relationship between the variables and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad4da93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['ApplicantIncome'], df['LoanAmount'])\n",
    "plt.xlabel('ApplicantIncome')\n",
    "plt.ylabel('LoanAmount')\n",
    "plt.title('Scatter Plot - ApplicantIncome vs. LoanAmount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469516b1",
   "metadata": {},
   "source": [
    "As you can see, some points are notably distant from the majority of the population, suggesting the presence of potential outliers. These outlying data points stand apart from the overall pattern, warranting further investigation to understand their significance and impact on the relationship between the two variables. Identifying and handling outliers is crucial for ensuring accurate data analysis and model performance. By visualizing the scatter plot, we can gain valuable insights into the data distribution and correlations, paving the way for effective decision-making and data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d42dd",
   "metadata": {},
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "Anomaly detection is a specific approach to detecting rare events, where the focus is on identifying instances that significantly deviate from the norm or normal behaviour. Anomalies can be caused by rare events, errors, or unusual patterns that are not typical in the dataset. This technique is particularly useful when there is limited or no labelled data for the rare events. Common anomaly detection algorithms include:\n",
    "\n",
    "- Unsupervised Methods: Techniques like Isolation Forest and One-Class SVM can be used to identify anomalies in data without requiring labelled examples of the rare event.\n",
    "- Semi-Supervised Methods: These approaches combine both normal and abnormal data during training but have only a limited number of labelled anomalies. Autoencoders and Variational Autoencoders are examples of semi-supervised anomaly detection algorithms.\n",
    "- Supervised Methods: If a small number of labelled anomalies are available, supervised learning algorithms like Random Forest, Support Vector Machines (SVM), and Neural Networks can be used for anomaly detection.\n",
    "\n",
    "Let’s understand these methods in detail with Python code examples.\n",
    "\n",
    "#### Unsupervised Method using Isolation Forest\n",
    "\n",
    "Isolation Forest is an efficient and effective algorithm used for anomaly detection in unsupervised learning scenarios. It works by isolating anomalies or rare events in the data by constructing isolation trees (random decision trees) that separate the anomalies from the majority of the normal data points. It was introduced by Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou in their 2008 paper titled “Isolation Forest.” Here are some key concepts and features of the Isolation Forest algorithm:\n",
    "\n",
    "- Random Partitioning: Isolation Forest uses a random partitioning strategy to create isolation trees. At each step of constructing a tree, a random feature is selected, and a random split value within the range of the selected feature’s values is chosen to create a node. This random partitioning leads to shorter paths for anomalies, making them easier to isolate from normal data points.\n",
    "- Path Length: The key idea behind Isolation Forest is that anomalies are isolated into smaller partitions with fewer data points, while normal data points are distributed more uniformly across larger partitions. The average path length of a data point to reach an anomaly in the tree is used as a measure of its “isolation.”\n",
    "- Anomaly Score: Based on the average path length, each data point is assigned an anomaly score. The anomaly score represents how easily the data point can be isolated or separated from the rest of the data. Shorter average path lengths correspond to higher anomaly scores, indicating that the data point is more likely to be an anomaly.\n",
    "- Contamination Parameter: The Isolation Forest algorithm has a hyperparameter called “contamination,” which represents the expected proportion of anomalies in the dataset. This parameter helps in setting the threshold for identifying anomalies. The contamination parameter can be set explicitly or as “auto,” which estimates the contamination based on the dataset’s size.\n",
    "\n",
    "Few advantages of Isolation Forest are as follows: \n",
    "\n",
    "- Isolation Forest is computationally efficient and scalable, making it suitable for large datasets. It does not require a large number of trees to achieve good performance, reducing the computational overhead. \n",
    "- The algorithm is relatively insensitive to the number of dimensions/features, which is particularly advantageous when dealing with high-dimensional datasets. Isolation Forest is an unsupervised learning algorithm, making it suitable for scenarios where labelled anomaly data is scarce or unavailable.\n",
    "Limitations of Isolation Forest: \n",
    "- Isolation Forest may not perform well on datasets with multiple clusters of anomalies or when anomalies are close to the majority of normal data points. \n",
    "- Like most unsupervised algorithms, Isolation Forest may produce false positives (normal data points misclassified as anomalies) and false negatives (anomalies misclassified as normal data points).\n",
    "\n",
    "Lets implement this approach in Python:\n",
    "\n",
    "1.\tImport Libraries: The code begins by importing the necessary libraries. Pandas is imported as pd to handle data in tabular format, and IsolationForest is imported from the sklearn.ensemble module for performing anomaly detection using the Isolation Forest algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6788de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35327103",
   "metadata": {},
   "source": [
    "2.\tExtract Numerical Features: A list numerical_features is defined, containing the names of the numerical columns to be used for anomaly detection. These columns are ‘ApplicantIncome’, ‘CoapplicantIncome’, and ‘LoanAmount’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb5cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c3627",
   "metadata": {},
   "source": [
    "3.\tCreate DataFrame for Anomaly Detection: A new DataFrame X_anomaly is created by extracting the columns specified in numerical_features from the original DataFrame df. This new DataFrame will be used for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cfb7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_anomaly = df[numerical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5c951",
   "metadata": {},
   "source": [
    "4.\tHandle Missing Values: To handle any missing values in the X_anomaly DataFrame, the fillna() method is used with the mean of each column. This ensures that any missing values are replaced with the mean value of their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858bb5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manmo\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n"
     ]
    }
   ],
   "source": [
    "X_anomaly.fillna(X_anomaly.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320fccf",
   "metadata": {},
   "source": [
    "5.\tInitialize Isolation Forest Model: The Isolation Forest model is initialized with IsolationForest(contamination=’auto’, random_state=42). The parameter contamination is set to ‘auto’, which means it will automatically detect the percentage of outliers in the dataset. The random_state is set to 42 to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4315c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "Isolation_forest = IsolationForest(contamination='auto', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896b4ff",
   "metadata": {},
   "source": [
    "6.\tFit the Model and Predict Anomalies: The Isolation Forest model is fitted to the X_anomaly data using the fit_predict() method. This method simultaneously fits the model to the data and predicts whether each data point is an outlier or not. The predictions are stored in the anomaly_predictions array.\n",
    "7.\tAdd Anomaly Predictions to Original Dataset: The anomaly_predictions array contains the predicted labels for each data point: -1 for anomalies (outliers) and 1 for inliers (non-outliers). These predictions are added as a new column ‘IsAnomaly’ to the original DataFrame df.\n",
    "8.\tDisplay Rows with Anomalies: Finally, the code filters the rows in the DataFrame df where ‘IsAnomaly’ is equal to -1, indicating the presence of outliers. The resulting DataFrame anomalies contains all the rows with anomalies, which can then be further analyzed or processed as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2a38c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>IsAnomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LP001020</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>12841</td>\n",
       "      <td>10968.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>N</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LP001028</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3073</td>\n",
       "      <td>8106.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LP001030</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>1299</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LP001046</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5955</td>\n",
       "      <td>5625.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LP001100</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>12500</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Loan_ID Gender Married Dependents Education Self_Employed  \\\n",
       "9   LP001020   Male     Yes          1  Graduate            No   \n",
       "12  LP001028   Male     Yes          2  Graduate            No   \n",
       "14  LP001030   Male     Yes          2  Graduate            No   \n",
       "21  LP001046   Male     Yes          1  Graduate            No   \n",
       "34  LP001100   Male      No         3+  Graduate            No   \n",
       "\n",
       "    ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "9             12841            10968.0       349.0             360.0   \n",
       "12             3073             8106.0       200.0             360.0   \n",
       "14             1299             1086.0        17.0             120.0   \n",
       "21             5955             5625.0       315.0             360.0   \n",
       "34            12500             3000.0       320.0             360.0   \n",
       "\n",
       "    Credit_History Property_Area Loan_Status  IsAnomaly  \n",
       "9              1.0     Semiurban           N         -1  \n",
       "12             1.0         Urban           Y         -1  \n",
       "14             1.0         Urban           Y         -1  \n",
       "21             1.0         Urban           Y         -1  \n",
       "34             1.0         Rural           N         -1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_predictions = Isolation_forest.fit_predict(X_anomaly)\n",
    "df['IsAnomaly'] = anomaly_predictions\n",
    "anomalies = df[df['IsAnomaly'] == -1]\n",
    "anomalies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55832c",
   "metadata": {},
   "source": [
    "### Semi-Supervised Methods Using Autoencoders\n",
    "\n",
    "Anomaly Detection using Autoencoders is an unsupervised learning approach that leverages neural networks to detect anomalies in data. Autoencoders are a type of neural network architecture designed to reconstruct the input data from a compressed representation. In anomaly detection, we exploit the fact that autoencoders struggle to reconstruct anomalous instances, making them useful for identifying unusual patterns or outliers.\n",
    "\n",
    "Autoencoders consist of two main components: the encoder and the decoder. The encoder compresses the input data into a lower-dimensional representation called the \"latent space,\" while the decoder tries to reconstruct the original input from this representation. The encoder and decoder are typically symmetric, and the network is trained to minimize the reconstruction error.\n",
    "\n",
    "In anomaly detection, we train the autoencoder on normal data without anomalies. Since the autoencoder learns to reconstruct normal data, it will be less capable of reconstructing anomalies, leading to higher reconstruction errors for anomalous instances. This property allows us to use the reconstruction error as an anomaly score.\n",
    "\n",
    "During training, we compare the original input (e.g., numerical features) to the reconstructed output. The difference between the two is the reconstruction error. A low reconstruction error indicates that the input is close to the normal data distribution, while a high reconstruction error suggests that the input is likely an anomaly.\n",
    "\n",
    "After training the autoencoder, we need to set a threshold to distinguish between normal and anomalous instances based on the reconstruction error. There are several methods to set the threshold, such as percentile-based or using validation data. The threshold will depend on the desired trade-off between false positives and false negatives, which can be adjusted based on the application's requirements.\n",
    "\n",
    "Autoencoders are flexible and can capture complex patterns in the data, making them suitable for high-dimensional data with non-linear relationships. They can handle both global and local anomalies, meaning they can detect anomalies that differ from the majority of the data points and anomalies within specific regions of the data. Autoencoders are capable of unsupervised learning, which is advantageous when labelled anomaly data is limited or unavailable. Like other unsupervised methods, autoencoders may produce false positives (normal data misclassified as anomalies) and false negatives (anomalies misclassified as normal data). They may struggle to detect anomalies that are very similar to the normal data, as the reconstruction error might not be significantly different.\n",
    "\n",
    "Lets see an example implementation of this approach using TensorFlow library and Loan Prediction dataset in Python.\n",
    "\n",
    "- Load Data: The code begins by importing necessary libraries and loading the dataset from the file 'train_loan_prediction.csv' using pd.read_csv().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f51a2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "df = pd.read_csv('train_loan_prediction.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508ea21",
   "metadata": {},
   "source": [
    "- Extract Numerical Features: The code defines a list numerical_features containing the names of the numerical columns to be used for anomaly detection. These columns are 'ApplicantIncome', 'CoapplicantIncome', and 'LoanAmount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e4ddf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b751e67",
   "metadata": {},
   "source": [
    "- Create DataFrame for Anomaly Detection: A new DataFrame X_anomaly is created by extracting the columns specified in numerical_features from the original DataFrame df. This new DataFrame will be used for anomaly detection.\n",
    "- Handle Missing Values: Any missing values in the X_anomaly DataFrame are replaced with the mean value of their respective columns using fillna() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "763feebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manmo\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n"
     ]
    }
   ],
   "source": [
    "X_anomaly = df[numerical_features]\n",
    "X_anomaly.fillna(X_anomaly.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675d777",
   "metadata": {},
   "source": [
    "- Standardize the Numerical Features: The numerical features in X_anomaly are standardized using StandardScaler(). Standardization scales the features to have zero mean and unit variance, which is important for training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42ae1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_indices = X_anomaly.index\n",
    "scaler = StandardScaler()\n",
    "X_anomaly_scaled = scaler.fit_transform(X_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fa16f",
   "metadata": {},
   "source": [
    "- Split Data into Training and Testing Sets: The standardized data is split into training (X_train) and testing (X_test) sets using train_test_split() function. The original indices of the data are also stored in original_indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c2c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, _, _ = train_test_split(X_anomaly_scaled, original_indices, test_size=0.2, random_state=42)   \n",
    "X_test_df = pd.DataFrame(X_test, columns=['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a558f0",
   "metadata": {},
   "source": [
    "- Build and Train Autoencoder Model: An autoencoder neural network model is constructed using TensorFlow's Keras API. The autoencoder is an unsupervised learning model designed to reconstruct the input data. It consists of an encoder and decoder, both composed of Dense layers. The model is trained using the fit() method with the mean squared error as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae11ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "39/39 [==============================] - 1s 3ms/step - loss: 1.2754\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.2592\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.2439\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.2294\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 0s 7ms/step - loss: 1.2155\n",
      "Epoch 6/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 1.2020\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.1891\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 1.1764\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 0s 994us/step - loss: 1.1639\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.1512\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 1.1387\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.1257\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.1130\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.1003\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 1.0880\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 1.0762\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.0650\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.0539\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.0435\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.0333\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 1.0236\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 1.0142\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 1.0052\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9965\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9880\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9798\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9717\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9637\n",
      "Epoch 29/50\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.9558\n",
      "Epoch 30/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9481\n",
      "Epoch 31/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.9412\n",
      "Epoch 32/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.9346\n",
      "Epoch 33/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9280\n",
      "Epoch 34/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9214\n",
      "Epoch 35/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9150\n",
      "Epoch 36/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.9085\n",
      "Epoch 37/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.9022\n",
      "Epoch 38/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8959\n",
      "Epoch 39/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8897\n",
      "Epoch 40/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8837\n",
      "Epoch 41/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8777\n",
      "Epoch 42/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.8718\n",
      "Epoch 43/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.8660\n",
      "Epoch 44/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8602\n",
      "Epoch 45/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8546\n",
      "Epoch 46/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8492\n",
      "Epoch 47/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8438\n",
      "Epoch 48/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8387\n",
      "Epoch 49/50\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.8338\n",
      "Epoch 50/50\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.8290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1989b106ac8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_anomaly.shape[1]\n",
    "encoding_dim = 2\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder_layer = Dense(input_dim, activation='sigmoid')(encoder_layer)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(X_anomaly_scaled, X_anomaly_scaled, epochs=50, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822c4b8",
   "metadata": {},
   "source": [
    "- Reconstruct Data and Calculate Reconstruction Error: The trained autoencoder is used to reconstruct the data points in X_test, and the reconstruction errors are calculated as the mean squared difference between the original and reconstructed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b31d9771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "X_test_reconstructed = autoencoder.predict(X_test)\n",
    "reconstruction_error_test = np.mean(np.square(X_test - X_test_reconstructed), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c38ea",
   "metadata": {},
   "source": [
    "- Define Threshold for Anomaly Detection: A threshold for anomaly detection is defined by calculating the 95th percentile of the reconstruction errors in X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92af0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(reconstruction_error_test, 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647688f",
   "metadata": {},
   "source": [
    "- Predict Anomalies: Anomalies are predicted by comparing the reconstruction errors against the threshold. If the reconstruction error for a data point is greater than the threshold, it is classified as an anomaly and assigned a value of 1 in anomaly_predictions, otherwise, it is assigned 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "408f2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_predictions = (reconstruction_error_test > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702177f",
   "metadata": {},
   "source": [
    "- Create a New DataFrame with Anomaly Predictions: A new DataFrame anomaly_df is created with the anomaly predictions and the corresponding index from X_test_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "775df037",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_df = pd.DataFrame({'IsAnomaly': anomaly_predictions}, index=X_test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a2a1e4",
   "metadata": {},
   "source": [
    "- Merge Anomaly Predictions with the Original DataFrame: The anomaly predictions are merged with the original DataFrame df using merge() method, adding the 'IsAnomaly' column to df.\n",
    "- Display Rows with Anomalies: The code checks if the 'IsAnomaly' column is present in df. If present, it displays the rows where 'IsAnomaly' is equal to 1, indicating the presence of anomalies. If not present, it prints \"No anomalies detected.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9b0ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(anomaly_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "if 'IsAnomaly' in df.columns:\n",
    "    # Display the rows with anomalies\n",
    "    anomalies = df[df['IsAnomaly'] == 1]\n",
    "    anomalies\n",
    "else:\n",
    "    print(\"No anomalies detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ede46b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>IsAnomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LP001052</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3717</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>LP001205</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2500</td>\n",
       "      <td>3796.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>LP001273</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>LP001310</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5695</td>\n",
       "      <td>4167.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>LP001316</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2958</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>LP001357</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3816</td>\n",
       "      <td>754.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>LP001431</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2137</td>\n",
       "      <td>8980.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Loan_ID  Gender Married Dependents Education Self_Employed  \\\n",
       "24   LP001052    Male     Yes          1  Graduate           NaN   \n",
       "60   LP001205    Male     Yes          0  Graduate            No   \n",
       "83   LP001273    Male     Yes          0  Graduate            No   \n",
       "89   LP001310    Male     Yes          0  Graduate            No   \n",
       "90   LP001316    Male     Yes          0  Graduate            No   \n",
       "104  LP001357    Male     NaN        NaN  Graduate            No   \n",
       "122  LP001431  Female      No          0  Graduate            No   \n",
       "\n",
       "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "24              3717             2925.0       151.0             360.0   \n",
       "60              2500             3796.0       120.0             360.0   \n",
       "83              6000             2250.0       265.0             360.0   \n",
       "89              5695             4167.0       175.0             360.0   \n",
       "90              2958             2900.0       131.0             360.0   \n",
       "104             3816              754.0       160.0             360.0   \n",
       "122             2137             8980.0       137.0             360.0   \n",
       "\n",
       "     Credit_History Property_Area Loan_Status  IsAnomaly  \n",
       "24              NaN     Semiurban           N        1.0  \n",
       "60              1.0         Urban           Y        1.0  \n",
       "83              NaN     Semiurban           N        1.0  \n",
       "89              1.0     Semiurban           Y        1.0  \n",
       "90              1.0     Semiurban           Y        1.0  \n",
       "104             1.0         Urban           Y        1.0  \n",
       "122             0.0     Semiurban           Y        1.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['IsAnomaly'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f05c5",
   "metadata": {},
   "source": [
    "### Supervised Methods Using Support Vector Machines (SVM):\n",
    "Support Vector Machines (SVM) are a powerful class of supervised learning algorithms commonly used for classification tasks. When applied to anomaly detection, SVMs prove to be effective in separating normal instances from anomalies by finding a hyperplane with maximum margin. Here is how SVM work under the hood:\n",
    "\n",
    "- Hyperplane Definition: In a two-dimensional space, a hyperplane is a flat, two-dimensional subspace. SVM aims to find a hyperplane that best separates the dataset into two classes — normal and anomalous. This hyperplane is positioned to maximize the margin, which is the distance between the hyperplane and the nearest data points of each class.\n",
    "- Decision Boundary: The hyperplane serves as a decision boundary that separates instances of one class from another. In a binary classification scenario, instances on one side of the hyperplane are classified as belonging to one class, and those on the other side are classified as belonging to the other class.\n",
    "- Kernel Trick: SVM can handle complex relationships in the data through the use of a kernel function. In many real-world scenarios, the relationship between features may not be linear. SVM addresses this by using a kernel function. This function transforms the input data into a higher-dimensional space, making it easier to find a hyperplane that effectively separates the classes. Commonly used kernel functions include the linear kernel (for linearly separable data), polynomial kernel, radial basis function (RBF) or Gaussian kernel, and sigmoid kernel. The choice of kernel depends on the nature of the data.\n",
    "- Optimal Hyperplane: SVM aims to find the hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points of each class. The larger the margin, the more robust and generalizable the model is likely to be. Support vectors are the data points that lie closest to the decision boundary. They play a crucial role in defining the optimal hyperplane and the margin. SVM focuses on these support vectors during training.\n",
    "\n",
    "Lets implement a Python example of SVM in anomaly detection using our Loan Prediction dataset:\n",
    "\n",
    "- Load Data: Let’s begin by importing necessary libraries and loading the dataset from the file 'train_loan_prediction.csv' using pd.read_csv().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af67d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "df = pd.read_csv('train_loan_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2223cd",
   "metadata": {},
   "source": [
    "- Data Pre-processing: We will do some basic data pre-processing tasks, including handling missing values. For this anomaly detection example, we simplify the analysis by excluding the categorical variables. In a more complex analysis, you might choose to encode and include these variables if they are deemed relevant to your specific anomaly detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a28f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'], axis=1)\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 0, 'N': 1})\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb26641",
   "metadata": {},
   "source": [
    "- Create Train-Test Split for the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83f4bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001dff0",
   "metadata": {},
   "source": [
    "- Standardize the features using StandardScaler from Scikit-learn to ensure that all features have the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80e4930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdcb37",
   "metadata": {},
   "source": [
    "- Train the One-Class SVM model for anomaly detection. Adjust the nu parameter based on the expected proportion of outliers in your dataset. The nu parameter represents an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. It essentially controls the proportion of outliers or anomalies the algorithm should consider. Choosing an appropriate value for nu is crucial, and it depends on the characteristics of your dataset and the expected proportion of anomalies. Here are some guidelines to help you select the nu parameter:\n",
    "- Understand the Nature of Anomalies: Assess the domain knowledge and characteristics of your dataset. Understand the expected proportion of anomalies. If anomalies are rare, a smaller value of nu might be appropriate.\n",
    "- Experiment with a Range of Values: Start by experimenting with a range of nu values, such as 0.01, 0.05, 0.1, 0.2, etc. You can adjust this range based on your understanding of the data.\n",
    "- Consider the Dataset Size: The size of your dataset can also influence the choice of nu. For larger datasets, a smaller value might be suitable, while for smaller datasets, a relatively larger value may be appropriate.\n",
    "- Balance False Positives and False Negatives: Depending on the application, you might prioritize minimizing false positives or false negatives. Adjust nu accordingly to achieve the desired balance.\n",
    "- We will implement an experiment with range of values for nu. We will specify a list of nu values that we want to experiment with. These values represent the upper bound on the fraction of margin errors and the lower bound of the fraction of support vectors in the One-Class SVM. We then create an empty list to store the mean decision function values for each nu value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cc22a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_values = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "mean_decision_function_values = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37370f31",
   "metadata": {},
   "source": [
    "- For each nu value in the list, train a One-Class SVM model with that nu value. Retrieve the decision function values for the test set and calculate the mean decision function value. Append the mean decision function value to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84f4d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nu in nu_values:\n",
    "    svm_model = OneClassSVM(nu=nu, kernel='rbf', gamma=0.1)\n",
    "    svm_model.fit(X_train_scaled)\n",
    "    decision_function_values=svm_model.decision_function(X_test_scaled)\n",
    "    mean_decision_function = np.mean(decision_function_values)\n",
    "    mean_decision_function_values.append(mean_decision_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd9610",
   "metadata": {},
   "source": [
    "- Identify the index of the nu value that corresponds to the highest mean decision function value. Then, retrieve the best nu value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cbfc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nu_index = np.argmax(mean_decision_function_values)\n",
    "best_nu = nu_values[best_nu_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe593dd4",
   "metadata": {},
   "source": [
    "- Create the final One-Class SVM model using the best nu value and train it on the scaled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46586d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(gamma=0.1, nu=0.2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = OneClassSVM(nu=best_nu, kernel='rbf', gamma=0.1)\n",
    "final_model.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c5f4f",
   "metadata": {},
   "source": [
    "- We now use this model to predict anomalies on the test dataset X_test_scaled. This line creates a binary representation of the predictions (y_pred) by mapping -1 to 1 (indicating anomalies) and any other value (typically 1) to 0 (indicating normal instances). This is done because the One-Class SVM often assigns -1 to anomalies and 1 to normal instances. We will store this in a new dataframe as ‘df_with_anomalies’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "749ded2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test_scaled)\n",
    "y_pred_binary = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "test_set_df = pd.DataFrame(data=X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "test_set_df['Anomaly_Label'] = y_pred_binary\n",
    "df_with_anomalies = pd.concat([df, test_set_df['Anomaly_Label']], axis=1, join='outer')\n",
    "df_with_anomalies['Anomaly_Label'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a2306",
   "metadata": {},
   "source": [
    "- Print the confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11f3dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.86      0.76        80\n",
      "           1       0.48      0.23      0.31        43\n",
      "\n",
      "    accuracy                           0.64       123\n",
      "   macro avg       0.58      0.55      0.54       123\n",
      "weighted avg       0.61      0.64      0.60       123\n",
      "\n",
      "Accuracy Score: 0.6422764227642277\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_binary))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e04334",
   "metadata": {},
   "source": [
    "- Print dataframe rows predicted as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c8a97c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>Anomaly_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>3200</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2071</td>\n",
       "      <td>754.0</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>4000</td>\n",
       "      <td>7750.0</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>39999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>4758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2882</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1668</td>\n",
       "      <td>3890.0</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>6083</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.842199</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>20667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.412162</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1800</td>\n",
       "      <td>2934.0</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>63337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>3180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>6096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>342.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>16667</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>13262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>2378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>3159</td>\n",
       "      <td>461.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>4106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "66              3200             2254.0  126.000000             180.0   \n",
       "109             2071              754.0   94.000000             480.0   \n",
       "135             4000             7750.0  290.000000             360.0   \n",
       "155            39999                0.0  600.000000             180.0   \n",
       "218             5000                0.0   72.000000             360.0   \n",
       "228             4758                0.0  158.000000             480.0   \n",
       "231             3716                0.0   42.000000             180.0   \n",
       "248             2882             1843.0  123.000000             480.0   \n",
       "250             1668             3890.0  201.000000             360.0   \n",
       "260             6083             4250.0  330.000000             360.0   \n",
       "284            20667                0.0  146.412162             360.0   \n",
       "300             1800             2934.0   93.000000             360.0   \n",
       "333            63337                0.0  490.000000             180.0   \n",
       "396             3180                0.0   71.000000             360.0   \n",
       "412             6096                0.0  218.000000             360.0   \n",
       "421             2720                0.0   80.000000             342.0   \n",
       "478            16667             2250.0   86.000000             360.0   \n",
       "509            13262                0.0   40.000000             360.0   \n",
       "568             2378                0.0    9.000000             360.0   \n",
       "575             3159              461.0  108.000000              84.0   \n",
       "610             4106                0.0   40.000000             180.0   \n",
       "\n",
       "     Credit_History  Loan_Status  Anomaly_Label  \n",
       "66         0.000000            1            1.0  \n",
       "109        1.000000            0            1.0  \n",
       "135        1.000000            1            1.0  \n",
       "155        0.000000            0            1.0  \n",
       "218        0.000000            1            1.0  \n",
       "228        1.000000            0            1.0  \n",
       "231        1.000000            0            1.0  \n",
       "248        1.000000            0            1.0  \n",
       "250        0.000000            1            1.0  \n",
       "260        0.842199            0            1.0  \n",
       "284        1.000000            1            1.0  \n",
       "300        0.000000            1            1.0  \n",
       "333        1.000000            0            1.0  \n",
       "396        0.000000            1            1.0  \n",
       "412        0.000000            1            1.0  \n",
       "421        0.000000            1            1.0  \n",
       "478        1.000000            0            1.0  \n",
       "509        1.000000            0            1.0  \n",
       "568        1.000000            1            1.0  \n",
       "575        1.000000            0            1.0  \n",
       "610        1.000000            0            1.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_anomalies[df_with_anomalies['Anomaly_Label'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c89ba2",
   "metadata": {},
   "source": [
    "### Data Augmentation and Resampling\n",
    "\n",
    "Class imbalance is a common issue in datasets with rare events. In our case, the 'Loan_Status' of 'Y' might be significantly higher than 'N.' Class imbalance can adversely affect the model's performance, as the model tends to be biased towards the majority class. To address this, we will explore two resampling techniques:\n",
    "\n",
    "- Oversampling: Increasing the number of instances in the minority class by generating synthetic samples. \n",
    "- Undersampling: Reducing the number of instances in the majority class to balance the class distribution. \n",
    "\n",
    "Let's discuss these resampling techniques in more detail.\n",
    "\n",
    "#### Oversampling using SMOTE (Synthetic Minority Over-sampling Technique) \n",
    "\n",
    "The Synthetic Minority Over-sampling Technique (SMOTE) is a widely used resampling method for addressing class imbalance in machine learning datasets, especially when dealing with rare events or minority classes. SMOTE helps to generate synthetic samples for the minority class by interpolating between existing minority class samples. This technique aims to balance the class distribution by creating additional synthetic instances, thereby mitigating the effects of class imbalance. In a dataset with class imbalance, the minority class contains significantly fewer instances than the majority class. This can lead to biased model training, where the model tends to favor the majority class and performs poorly on the minority class.\n",
    "Here are the key steps of a SMOTE Algorithm:\n",
    "\n",
    "1.\tIdentify Minority Class Instances: The first step of SMOTE is to identify the instances belonging to the minority class.\n",
    "2.\tSelecting Nearest Neighbors: For each minority class instance, SMOTE selects its k nearest neighbours (commonly chosen through the k-nearest neighbours algorithm). These neighbours are used to create synthetic samples.\n",
    "3.\tCreating Synthetic Samples: For each minority class instance, SMOTE generates synthetic samples along the line connecting the instance to its k nearest neighbours in the feature space. The synthetic samples are created by adding a random fraction (usually between 0 and 1) of the feature differences between the instance and its neighbours. This process effectively introduces variability to the synthetic samples.\n",
    "4.\tCombining with the Original Data: The synthetic samples are combined with the original minority class instances, resulting in a resampled dataset with a more balanced class distribution.\n",
    "\n",
    "SMOTE helps to address class imbalance without discarding any data, as it generates synthetic samples rather than removing instances from the majority class. It increases the information available to the model, potentially improving the model's ability to generalize to the minority class. SMOTE is straightforward to implement and is available in popular libraries such as imbalanced-learn in Python.\n",
    "\n",
    "While SMOTE is effective in many cases, it might not always perform optimally for highly imbalanced datasets or datasets with complex decision boundaries. Generating too many synthetic samples can lead to overfitting on the training data, so it is crucial to choose an appropriate value for the number of nearest neighbours (k). SMOTE may introduce some noise and may not be as effective if the minority class is too sparse or scattered in the feature space. SMOTE can be combined with other techniques, such as undersampling the majority class or using different resampling ratios, to achieve better performance. It is essential to evaluate the model's performance on appropriate metrics (e.g., precision, recall, F1-score) to assess the impact of SMOTE and other techniques on the model's ability to detect rare events.\n",
    "\n",
    "Lets implement this approach in Python:\n",
    "\n",
    "- Load Data: The code starts by importing necessary libraries and loading the Loan Prediction dataset from the file 'train_loan_prediction.csv' using pd.read_csv().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a9670e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "df = pd.read_csv('train_loan_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522c17a",
   "metadata": {},
   "source": [
    "- Mapping Target Variable: The 'Loan_Status' column in the dataset contains categorical values 'Y' and 'N', which represent loan approval ('Y') and rejection ('N'). To convert this categorical target variable into numerical format, 'Y' is mapped to 1 and 'N' is mapped to 0 using the map() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ece05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137067c5",
   "metadata": {},
   "source": [
    "- Handle Missing Values: The code applies mean imputation to all columns with missing values in the dataset using the fillna() method. This ensures that any missing values in the dataset are replaced with the mean value of their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79eb2cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a2cd8",
   "metadata": {},
   "source": [
    "- Exclude Non-Numerical Columns: The code selects only the numerical columns from the dataset to build the feature set X. The select_dtypes() method is used to include only columns with data types 'float' and 'int', while excluding non-numerical columns such as 'Loan_Status' and 'Loan_ID'. The target variable y is set to 'Loan_Status'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bed6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=[float, int]).columns\n",
    "X = df[numerical_columns].drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67a356",
   "metadata": {},
   "source": [
    "- Split Data into Training and Testing Sets: The data is split into training (X_train, y_train) and testing (X_test, y_test) sets using train_test_split() function from scikit-learn. The training set consists of 80% of the data, while the testing set contains 20% of the data. The random_state parameter is set to 42 to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0418838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a851c",
   "metadata": {},
   "source": [
    "- Instantiate SMOTE: Here, SMOTE is instantiated with SMOTE(random_state=42). SMOTE is then applied to the training data using the fit_resample() method. This method oversamples the minority class (loan rejection) by generating synthetic samples, creating a balanced dataset. The resulting resampled data is stored in X_train_resampled and y_train_resampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e8a32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d14643",
   "metadata": {},
   "source": [
    "- Train Random Forest Classifier: A Random Forest Classifier is instantiated with RandomForestClassifier(random_state=42). The classifier is trained on the resampled data (X_train_resampled, y_train_resampled) using the fit() method. The trained classifier is used to make predictions on the test data (X_test) using the predict() method. The predictions are stored in y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "376c5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb7018",
   "metadata": {},
   "source": [
    "- The classification report is generated using the classification_report() function from scikit-learn. The classification report provides precision, recall, F1-score, and support for each class (loan approval and rejection) based on the predictions (y_pred) and the true labels (y_test) from the test set. The classification report is initially returned as a dictionary format. The code converts this dictionary to a DataFrame clf_report using pd.DataFrame(), making it easier to work with the data. The DataFrame clf_report is transposed using the .T attribute to have classes ('0' and '1') as rows and evaluation metrics (precision, recall, F1-score, and support) as columns. This transpose provides a more convenient and readable format for further analysis or presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "030e5da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.832370</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.764228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.753763</td>\n",
       "      <td>0.705814</td>\n",
       "      <td>0.717555</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.759909</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.752093</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.733333  0.511628  0.602740   43.000000\n",
       "1              0.774194  0.900000  0.832370   80.000000\n",
       "accuracy       0.764228  0.764228  0.764228    0.764228\n",
       "macro avg      0.753763  0.705814  0.717555  123.000000\n",
       "weighted avg   0.759909  0.764228  0.752093  123.000000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))\n",
    "clf_report = clf_report.T\n",
    "clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa97e4",
   "metadata": {},
   "source": [
    "#### Undersampling using RandomUnderSampler\n",
    "\n",
    "Handling class imbalance with RandomUnderSampler is an effective approach to address the challenge of imbalanced datasets, where one class significantly outweighs the other class(es). In such cases, traditional machine learning algorithms may struggle to learn from the data and tend to be biased towards the majority class, leading to poor performance on the minority class or rare events.\n",
    "\n",
    "RandomUnderSampler is a resampling technique that aims to balance the class distribution by randomly removing instances from the majority class until the class proportions become more balanced. By reducing the number of instances in the majority class, RandomUnderSampler ensures that the minority class is represented more proportionately, making it easier for the model to detect and learn patterns related to the rare events.\n",
    "\n",
    "Here are some key points about handling class imbalance with RandomUnderSampler:\n",
    "\n",
    "- Resampling for Class Balance: RandomUnderSampler is a type of data-level resampling method. Data-level resampling techniques involve manipulating the training data to balance the class distribution. In RandomUnderSampler, instances from the majority class are randomly selected and removed, resulting in a smaller dataset with a balanced class distribution.\n",
    "- Preserving Minority Class Information: Unlike some other undersampling techniques that merge instances or create synthetic samples, RandomUnderSampler directly removes instances from the majority class without altering the minority class instances. This approach helps preserve the information from the minority class, making it easier for the model to focus on learning the patterns associated with the rare events.\n",
    "- Potential Information Loss: One potential drawback of RandomUnderSampler is the loss of information from the majority class. By removing instances randomly, some informative instances may be discarded, potentially leading to a reduction in the model's ability to generalize on the majority class.\n",
    "- Computationally Efficient: RandomUnderSampler is computationally efficient since it simply involves randomly removing instances from the majority class. This makes it faster compared to some other resampling methods.\n",
    "- Choosing the Right Resampling Technique: While RandomUnderSampler can be effective in certain scenarios, it might not always be the best choice, especially if the majority class contains important patterns and information. Careful consideration of the problem and dataset characteristics is crucial when selecting the appropriate resampling technique.\n",
    "- Combining with Other Techniques: In practice, RandomUnderSampler can be used in combination with other techniques. For example, one can apply RandomUnderSampler first and then use RandomOverSampler (oversampling) to further balance the class distribution. This approach helps in achieving a more balanced representation of both classes.\n",
    "- Evaluation and Model Selection: When handling class imbalance, it is essential to evaluate the model's performance on relevant metrics such as precision, recall, F1-score, and area under the ROC curve (AUC-ROC). These metrics provide a comprehensive assessment of the model's ability to handle rare events and edge cases.\n",
    "\n",
    "Lets implement this approach using Python.\n",
    "\n",
    "Load Data: The code begins by importing necessary libraries and loading the Loan Prediction dataset from the file 'train_loan_prediction.csv' using pd.read_csv(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05bbc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "df = pd.read_csv('train_loan_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0e3f0",
   "metadata": {},
   "source": [
    "The 'Loan_Status' column in the dataset contains categorical values 'Y' and 'N', which represent loan approval ('Y') and rejection ('N'). To convert this categorical target variable into numerical format, 'Y' is mapped to 1 and 'N' is mapped to 0 using the map() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ea8b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5349ad7",
   "metadata": {},
   "source": [
    "Below code applies mean imputation to all columns with missing values in the dataset using the fillna() method. This ensures that any missing values in the dataset are replaced with the mean value of their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61c16e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cea9c5",
   "metadata": {},
   "source": [
    "The code selects only the numerical columns from the dataset to build the feature set X. The select_dtypes() method is used to include only columns with data types 'float' and 'int', while excluding non-numerical columns such as 'Loan_Status' and 'Loan_ID'. The target variable y is set to 'Loan_Status'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29b5b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=[float, int]).columns\n",
    "X = df[numerical_columns].drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e71fc0",
   "metadata": {},
   "source": [
    "The data is split into training (X_train, y_train) and testing (X_test, y_test) sets using train_test_split() function from scikit-learn. The training set consists of 80% of the data, while the testing set contains 20% of the data. The random_state parameter is set to 42 to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ce6fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce7d87",
   "metadata": {},
   "source": [
    "We then instantiate RandomUnderSampler and apply to the training data using the fit_resample() method. This method undersamples the majority class (loan approval) to create a balanced dataset. The resulting resampled data is stored in X_train_resampled and y_train_resampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42d99454",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae945d",
   "metadata": {},
   "source": [
    "A Random Forest Classifier is then trained on the resampled data (X_train_resampled, y_train_resampled) using the fit() method. The trained classifier is used to make predictions on the test data (X_test) using the predict() method. The predictions are stored in y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c14079b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.835443</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.780488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.758631</td>\n",
       "      <td>0.761337</td>\n",
       "      <td>0.759922</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.781737</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.781059</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.681818  0.697674  0.689655   43.000000\n",
       "1              0.835443  0.825000  0.830189   80.000000\n",
       "accuracy       0.780488  0.780488  0.780488    0.780488\n",
       "macro avg      0.758631  0.761337  0.759922  123.000000\n",
       "weighted avg   0.781737  0.780488  0.781059  123.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))\n",
    "clf_report = clf_report.T\n",
    "clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ef679",
   "metadata": {},
   "source": [
    "The model shows decent performance for both classes, with higher precision, recall, and F1-score for class '1' compared to class '0'.The weighted average considers the imbalance in class distribution, providing a more representative measure of overall performance. The accuracy of 78.4% suggests that the model correctly predicted the class for approximately 78.4% of instances in the test set.\n",
    "In the next section, let's understand Cost-Sensitive Learning and explore its crucial role in scenarios where rare events bear significant consequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf895cc7",
   "metadata": {},
   "source": [
    "### Cost Sensitive Learning\n",
    "\n",
    "Cost-sensitive learning is a machine learning approach that takes into account the costs associated with misclassifications of different classes during the model training process. In traditional machine learning, the focus is on maximizing overall accuracy, but in many real-world scenarios, misclassifying certain classes can have more severe consequences than misclassifying others.\n",
    "\n",
    "For example, in a medical diagnosis application, misdiagnosing a severe disease as not present (false negative) could have more significant consequences than misdiagnosing a mild condition as present (false positive). In fraud detection, incorrectly flagging a legitimate transaction as fraudulent (false positive) might inconvenience the customer, while failing to detect actual fraudulent transactions (false negative) could lead to significant financial losses.\n",
    "\n",
    "Cost-sensitive learning addresses these imbalances in costs by assigning different misclassification costs to different classes. By incorporating these costs into the training process, the model is encouraged to prioritize minimizing the overall misclassification cost rather than simply optimizing accuracy.\n",
    "\n",
    "There are several approaches to implement cost-sensitive learning:\n",
    "\n",
    "- Modifying Loss Functions: The loss function used during model training can be modified to incorporate class-specific misclassification costs. The goal is to minimize the expected cost, which is a combination of the misclassification costs and the model's predictions.\n",
    "- Class Weights: Another approach is to assign higher weights to the minority class or the class with higher misclassification costs. This technique can be applied to various classifiers, such as decision trees, random forests, and support vector machines, to emphasize learning from the minority class.\n",
    "- Sampling Techniques: In addition to assigning weights, resampling techniques like oversampling the minority class or undersampling the majority class can also be used to balance the class distribution and improve the model's ability to learn from rare events.\n",
    "- Threshold Adjustment: By adjusting the classification threshold, we can control the trade-off between precision and recall, allowing us to make predictions that are more sensitive to the minority class.\n",
    "- Ensemble Methods: Ensemble methods, such as cost-sensitive boosting, combine multiple models to focus on hard-to-classify instances and assign higher weights to misclassified samples.\n",
    "\n",
    "Cost-sensitive learning is especially important in scenarios where the class imbalance is severe and the consequences of misclassification are critical. By taking into account the costs associated with different classes, the model can make more informed decisions and improve overall performance in detecting rare events and handling edge cases.\n",
    "\n",
    "It is important to note that cost-sensitive learning requires careful consideration of the cost matrix, as incorrectly specified costs can lead to unintended results. Proper validation and evaluation of the model on relevant metrics, considering the real-world costs, are crucial to ensure the effectiveness and reliability of cost-sensitive learning algorithms.\n",
    "\n",
    "To demonstrate cost-sensitive learning using the Loan Prediction dataset in Python.\n",
    "\n",
    "- Load the required libraries and dataset using pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e716e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "df = pd.read_csv('train_loan_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e066076",
   "metadata": {},
   "source": [
    "- We now need to perform data pre-processing to handle missing values and to convert target variable to numeric data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f81e8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc124e",
   "metadata": {},
   "source": [
    "- For this example we will use only numeric columns. We will then split the dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19376784",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=[float, int]).columns\n",
    "X = df[numerical_columns].drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539fd93",
   "metadata": {},
   "source": [
    "- We first calculate the class weights based on the inverse of class frequencies in the training data. The higher the frequency of a class, the lower its weight, and vice versa. This way, the model assigns higher importance to the minority class (rare events) and is more sensitive to its correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e956791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = dict(1 / y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf7308",
   "metadata": {},
   "source": [
    "- Next, we train the Random Forest Classifier with the class_weight parameter set to the calculated class weights. This modification allows the classifier to consider the class weights during the training process, effectively implementing cost-sensitive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "067a5243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 3.295302013422819,\n",
       "                                     1: 1.4356725146198832},\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444c60a",
   "metadata": {},
   "source": [
    "- After training the model, we make predictions on the test data and evaluate the classifier's performance using the classification report, which provides precision, recall, F1-score, and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "950b5051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.836158</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.764228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.766059</td>\n",
       "      <td>0.695058</td>\n",
       "      <td>0.707934</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.765104</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.746506</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.769231  0.465116  0.579710   43.000000\n",
       "1              0.762887  0.925000  0.836158   80.000000\n",
       "accuracy       0.764228  0.764228  0.764228    0.764228\n",
       "macro avg      0.766059  0.695058  0.707934  123.000000\n",
       "weighted avg   0.765104  0.764228  0.746506  123.000000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))\n",
    "clf_report = clf_report.T\n",
    "clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87475966",
   "metadata": {},
   "source": [
    "In cost-sensitive learning, you would typically define a cost matrix that quantifies the misclassification costs for each class and use it to guide the model's training. The results from the classification report can help you identify areas where adjustments may be needed to align the model with the specific cost considerations in your application. A cost matrix is especially useful in situations where the costs of false positives and false negatives are not equal. If the cost of false positives is higher, consider raising the decision threshold. If the cost of false negatives is higher, consider lowering the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8aec1",
   "metadata": {},
   "source": [
    "### Ensemble Techniques\n",
    "\n",
    "Ensemble techniques are powerful methods used to improve the performance of machine learning models, particularly in scenarios with imbalanced datasets, rare events, and edge cases. These techniques combine multiple base models to create a more robust and accurate final prediction. Let's discuss some of the popular ensemble techniques.\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble technique that creates multiple bootstrap samples (random subsets with replacement) from the training data and trains a separate base model on each sample. The final prediction is obtained by averaging or voting the predictions of all base models. Bagging is particularly useful when dealing with high variance and complex models, as it reduces overfitting and enhances the model's generalization ability. Here are the key concepts associated with bagging:\n",
    "\n",
    "1.\tBootstrap Sampling: The bagging process begins by creating multiple random subsets of the training data through a process called bootstrap sampling. Bootstrap sampling involves randomly selecting data points from the original dataset with replacement. As a result, some data points may appear more than once in a subset, while others may be left out.\n",
    "2.\tBase Model Training: For each bootstrap sample, a base model (learner) is trained independently on that particular subset of the training data. The base models can be any machine learning algorithm, such as decision trees, random forests, or support vector machines.\n",
    "3.\tAggregating Predictions: Once all base models are trained, they are used to make predictions on new, unseen data. For classification tasks, the final prediction is typically determined by majority voting, where the class that receives the most votes across the base models is chosen. In regression tasks, the final prediction is obtained by averaging the predictions from all base models.\n",
    "\n",
    "Few of the benefits of bagging are:\n",
    "- Variance Reduction: Bagging helps reduce variance in the model by combining predictions from multiple models trained on different subsets of the data. This results in a more stable and robust model.\n",
    "- Overfitting Prevention: By training each base model on different subsets of the data, bagging prevents individual models from overfitting to noise in the training set.\n",
    "- Model Generalization: Bagging improves the model's generalization ability by reducing bias and variance, leading to better performance on unseen data.\n",
    "- Parallelism: Since the base models are trained independently, bagging is amenable to parallel processing, making it computationally efficient.\n",
    "\n",
    "Example of Bagging using Random Forest: Random Forest is a popular example of the bagging technique. In Random Forest, the base models are decision trees, and the predictions from multiple decision trees are combined to make the final prediction.\n",
    "\n",
    "Here's an example of implementing bagging using Random Forest in Python on Loan Prediction dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c025683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.772358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.774306</td>\n",
       "      <td>0.706686</td>\n",
       "      <td>0.720455</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.773261</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.756689</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.777778  0.488372  0.600000   43.000000\n",
       "1              0.770833  0.925000  0.840909   80.000000\n",
       "accuracy       0.772358  0.772358  0.772358    0.772358\n",
       "macro avg      0.774306  0.706686  0.720455  123.000000\n",
       "weighted avg   0.773261  0.772358  0.756689  123.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv('train_loan_prediction.csv')\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "numerical_columns = df.select_dtypes(include=[float, int]).columns\n",
    "X = df[numerical_columns].drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))\n",
    "clf_report = clf_report.T\n",
    "clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f7c85",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Boosting is an ensemble technique that builds base models sequentially, with each subsequent model focusing on the misclassified instances of the previous model. It assigns higher weights to misclassified instances, thus giving more attention to rare events. Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost. Boosting aims to create a strong learner by combining weak learners iteratively.\n",
    "\n",
    "Here's how boosting works:\n",
    "\n",
    "1.\tBase Model Training: Boosting starts by training a base model (also known as a weak learner) on the entire training dataset. Weak learners are usually simple models with limited predictive power, such as decision stumps (a decision tree with a single split).\n",
    "2.\tWeighted Training: After the first model is trained, the data points that were misclassified by the model are assigned higher weights. This means that the subsequent model will pay more attention to those misclassified data points, attempting to correct their predictions.\n",
    "3.\tIterative Training: Boosting follows an iterative approach. For each iteration (or boosting round), a new weak learner is trained on the updated training data with adjusted weights. The weak learners are then combined to create a strong learner, which improves its predictive performance compared to the individual weak learners.\n",
    "4.\tWeighted Voting: During the final prediction, the weak learners' predictions are combined with weighted voting, where the models with higher accuracy have more influence on the final prediction. This allows the boosting algorithm to focus on difficult-to-classify instances and improve the model's sensitivity to rare events.\n",
    "\n",
    "Benefits of Boosting are as follows: \n",
    "\n",
    "- Increased Accuracy: Boosting improves the model's accuracy by focusing on the most challenging instances in the dataset and refining predictions over multiple iterations.\n",
    "- Robustness: Boosting reduces the model's sensitivity to noise and outliers in the data by iteratively adjusting the weights and learning from previous mistakes.\n",
    "- Model Adaptation: Boosting adapts well to different types of data and can handle complex relationships between features and the target variable.\n",
    "- Ensemble Diversity: Boosting creates a diverse ensemble of weak learners, which results in better generalization and reduced overfitting.\n",
    "\n",
    "Example of Boosting using AdaBoost: \n",
    "\n",
    "AdaBoost (Adaptive Boosting) is a popular boosting algorithm that is commonly used in practice. In AdaBoost, the base models are typically decision stumps, and the model's weights are adjusted after each iteration to emphasize misclassified instances.\n",
    "Here's an example of implementing boosting using AdaBoost in Python:\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fef2924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.752577</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.824859</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.747967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.741673</td>\n",
       "      <td>0.677180</td>\n",
       "      <td>0.687792</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.744953</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.729023</td>\n",
       "      <td>123.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.730769  0.441860  0.550725   43.000000\n",
       "1              0.752577  0.912500  0.824859   80.000000\n",
       "accuracy       0.747967  0.747967  0.747967    0.747967\n",
       "macro avg      0.741673  0.677180  0.687792  123.000000\n",
       "weighted avg   0.744953  0.747967  0.729023  123.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv('train_loan_prediction.csv')\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=[float, int]).columns\n",
    "X = df[numerical_columns].drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = AdaBoostClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))\n",
    "clf_report = clf_report.T\n",
    "clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a49f6",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "Stacking is an advanced ensemble learning technique that combines the predictions of multiple base models by training a meta-model on their outputs. Stacking aims to leverage the strengths of different base models to create a more accurate and robust final prediction. It is a form of \"learning to learn\" where the meta-model learns how to best combine the predictions of the base models. The base models act as \"learners,\" and their predictions become the input features for the meta-model, which makes the final prediction. Stacking can often improve performance by capturing complementary patterns from different base models. \n",
    "Below is the model methodology:\n",
    "\n",
    "1.\tBase Model Training: The stacking process starts by training multiple diverse base models on the training dataset. These base models can be different types of machine learning algorithms or even the same algorithm with different hyperparameters.\n",
    "2.\tBase Model Predictions: Once the base models are trained, they are used to make predictions on the same training data (in-sample predictions) or a separate validation dataset (out-of-sample predictions).\n",
    "3.\tMeta-Model Training: The predictions from the base models are then combined to create a new dataset that serves as the input for the meta-model. Each base model's predictions become a new feature in this dataset. The meta-model is trained on this new dataset along with the true target labels.\n",
    "4.\tFinal Prediction: During the final prediction phase, the base models make predictions on the new, unseen data. These predictions are then used as input features for the meta-model, which makes the final prediction.\n",
    "\n",
    "Benefits of Stacking: \n",
    "\n",
    "- Improved Predictive Performance: Stacking leverages the complementary strengths of different base models, potentially leading to better overall predictive performance compared to using individual models.\n",
    "- Reduction of Bias and Variance: Stacking can reduce the model's bias and variance by combining multiple models, leading to improved generalization.\n",
    "- Flexibility: Stacking allows the use of diverse base models, making it suitable for various types of data and problems.\n",
    "- Ensemble Diversity: Stacking creates a diverse ensemble by using various base models, which can help prevent overfitting.\n",
    "\n",
    "Here's an example of implementing stacking using scikit-learn in Python using Loan Prediction dataset.\n",
    "\n",
    "- We first import required libraries and load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73550b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "df = pd.read_csv('train_loan_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a80f25",
   "metadata": {},
   "source": [
    "We now need to perform data pre-processing to handle missing values and to convert target variable to numeric data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "379b4da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f353c68",
   "metadata": {},
   "source": [
    "For simplicity we will use only numeric columns for this example. We will then split the dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "371abd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=[float, int]).columns\n",
    "X = df[numerical_columns].drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0da11f",
   "metadata": {},
   "source": [
    "Instantiate and Train Base Models: Two base models, RandomForestClassifier and GradientBoostingClassifier, are instantiated with RandomForestClassifier(random_state=42) and GradientBoostingClassifier(random_state=42) respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c2bf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_1 = RandomForestClassifier(random_state=42)\n",
    "base_model_2 = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84802aa",
   "metadata": {},
   "source": [
    "These base models are trained on the training data (X_train, y_train) using the fit() method as below. The trained base models are used to make predictions on the test data (X_test) using the predict() method. The predictions from both base models are stored in pred_base_model_1 and pred_base_model_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc5f4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_1.fit(X_train, y_train)\n",
    "base_model_2.fit(X_train, y_train)\n",
    "pred_base_model_1 = base_model_1.predict(X_test)\n",
    "pred_base_model_2 = base_model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c7a56",
   "metadata": {},
   "source": [
    "Create New Dataset for Stacking: A new dataset stacking_X_train is created by combining the predictions from the base models (pred_base_model_1 and pred_base_model_2). This new dataset will be used as input features for the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4325554",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_X_train = pd.DataFrame({\n",
    "    'BaseModel1': pred_base_model_1,\n",
    "    'BaseModel2': pred_base_model_2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a53039",
   "metadata": {},
   "source": [
    "Instantiate and Train Meta-Model: A meta-model, Logistic Regression in this case, is instantiated with LogisticRegression(). The meta-model is trained on the new dataset (stacking_X_train) and the true labels from the test set (y_test) using the fit() method. The meta-model learns to combine the predictions of the base models and make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "03e5f91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stacking_X_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd169e",
   "metadata": {},
   "source": [
    "Create Unseen Data for Demonstration and make predictions: For demonstration purposes, a new sample of unseen data (new_unseen_data) is created by randomly selecting 20% of the test data (X_test) using sample() method. The base models are used to make predictions on the new, unseen data (new_unseen_data) using the predict() method. The predictions from both base models for the new data are stored in new_pred_base_model_1 and new_pred_base_model_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be629ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unseen_data = X_test.sample(frac=0.2, random_state=42)\n",
    "new_pred_base_model_1 = base_model_1.predict(new_unseen_data)\n",
    "new_pred_base_model_2 = base_model_2.predict(new_unseen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75df4c",
   "metadata": {},
   "source": [
    "Create New Dataset for Stacking with Unseen Data: A new dataset stacking_new_unseen_data is created by combining the predictions from the base models (new_pred_base_model_1 and new_pred_base_model_2) for the new, unseen data. This new dataset will be used as input features for the meta-model to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ea35f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_new_unseen_data = pd.DataFrame({\n",
    "    'BaseModel1': new_pred_base_model_1,\n",
    "    'BaseModel2': new_pred_base_model_2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8f070",
   "metadata": {},
   "source": [
    "Make Final Prediction using Meta-Model: The meta-model (Logistic Regression) is used to make the final prediction on the new, unseen data (stacking_new_unseen_data) using the predict() method. The final_prediction variable holds the predicted classes (0 or 1) based on the meta-model's decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "91982eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction = meta_model.predict(stacking_new_unseen_data)\n",
    "final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d9420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
